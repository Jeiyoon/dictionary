1. Unfortunate things always happen as a corollary to Murphy’s Law.

2. Concurrent installations of multiple editions of applications with interruption-free rollouts and rollback capability

3. the model that is pretrained using standard supervised learning on all tasks is unable to adequately adapt 
with so few datapoints without catastrophic overfitting.

4. MAML optimizes the parameters such that they lie in a region that is amenable to fast adaptation.

5. We follow the experimental protocol proposed by Vinyal et al. (2016)

6. locomotion tasks with MoJoCo simulator

7. He might want to add a caveat 

8. In conversation he lacks any trace of self-importance, and defers constantly to the interlocutor.

9. postulate the inherent goodness of man 

10. She pointed out the salient features of the new design.

11. You can’t lump all Asian languages together

12. you pick the corresponding task which corresponds to this sort of multiplicative gating 
where you're just selecting which of the outputs you're going to produce based of your task index .

13. The drug is not harmful per se, but is dangerous when taken with alcohol.

14. All things taken together, we opt for the Wiki-based BPE model.

15. discordant views

16. Deep network that can read in an entire dataset and make predictions for new datapoints

17. The figures were obtained by extrapolating from past trends.

18. 
- Bi-level optimization can exhibit instabilities
- mental/emotional instability

19. 
- What is the takeaway?
- I don't think that should be your biggest takeaway

20. Language modeling can be seen as the ideal source task and a counterpart of ImageNet for NLP

21. Now let’s look at another facet of the problem.

22. A pretrained LM can be easily adapted to the idiosyncrasies of a target task

23. The full LM is slanted triangular learning rates (STLR) to learn task-specific features

24. We add a layer at a time to the set of 'thawed' layers, rather than only training a single layer at a time.

25. 
- Tuning these weights by hand is a difficult and expensive process, making MTL prohibitive in practice.
- prohibitive legislation

26. According to the epistemic view, the outcomes of our actions are a key source of evidence about what goes on in our heads.

27. aleatory contract

28. For classification, we often squash the model output through a softmax function, and sample from the resulting probability vector

29. This new evidence obviates the need for any further enquiries

30. 
- The committee will need time to assimilate this report.
- the ability of a memory-augmented neural network assimilate new date

31. Writing to memory then occurs in accordance with the computed vector of write weights

32. the aptly named Grand Hotel

33. Learning quickly is a hallmark of human intelligence.

34. You mustn't allude to his wife's death when you meet him.

35. In low data regimes, non-parametric methods are simple, work well

36. Toward a Desirable Policy for Preservation of Prototypical Structure of Historic Cities

37. With hindsight it is easy to say they should not have released him.

38. each task for neta-learning is constructed "on the fly" during the meta-training stage, commonly described as an episode

39. We emphasize that we need to sample without replacement to optimize the generalization error.

40.
- an attempt to reconcile the need for industrial development with concern for the environment
- He could not reconcile himself to the prospect of losing her

41. this is often called "Amortized variational inference" and that we're having an inference network that's predicting 
what the variational distribution will be as basically amortizing the process of estimating that distribution

42. "Disclaimer": These topics are at the bleeding edge of research

43. What is the rationale behind these new exams?

44. Model can memorize the canonical orientations of the training objects

45. places precedence on using information from A over storing info in B

46. Living conditions are vastly different from those pertaining in their country of origin.

47. the cohesion of the nuclear family

48. It is possible to discern a number of different techniques in her work

49. 
- We caught him eavesdropping outside the window.
- implicit daa augmentation, attention focusing, eavesdropping, representation, and regularization

50. Work on the building was impeded by severe weather

51. The self-attention layer is followed by a fully connected feed-forward layer, which is applied to each position separately and identically

52. 
- The report is a masterpiece of brevity
- the brevity of human life

53. The second last layer is used since we need a more general contextualized representation 
and the embeddings corresponding to the last layer is more tuned towards the downstream tasks.

54. Now we come to the crux of the matter.

55. 
